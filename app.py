import streamlit as st
from langchain_community.llms import Tongyi
from streamlit_option_menu import option_menu
from dashscope import Generation
import datetime
import base64
from langchain.prompts import ChatPromptTemplate

API_KEY = 'sk-XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX'

menu0 = "é¦–é¡µ"
menu1 = "èŠå¤©æœºå™¨äºº"
menu2 = "ç¿»è¯‘æœºå™¨äºº"

st.sidebar.title("ğŸŒ¼ Tools ğŸŒ¼")

with st.sidebar:
    menu = option_menu("åŠŸèƒ½åˆ†ç±»", [menu0, menu1, menu2],
                       icons=['house', "list-task", "translate", ],
                       # menu_icon="cast",
                       default_index=0)


def main():
    if menu == menu0:
        showmenu0()

    if menu == menu1:
        showLLMChatbot()

    if menu == menu2:
        showmenu2()


def get_image_as_base64(file_path):
    with open(file_path, "rb") as img_file:
        return base64.b64encode(img_file.read()).decode()


def showmenu0():
    image_path = "E:/pycharm/shengchanshixi/huahui/pictures/flowe.jpg"
    base64_image = get_image_as_base64(image_path)

    # ä½¿ç”¨ CSS è®¾ç½®å›¾ç‰‡åœ†è§’
    st.markdown(
        f"""
        <style>
        .round-corner-image {{
            display: block;
            margin-left: auto;
            margin-right: auto;
            width: 100%;
            border-radius: 15px;
        }}
        </style>
        <img src="data:image/jpg;base64,{base64_image}" class="round-corner-image" />
        """,
        unsafe_allow_html=True
    )
    st.title(" æ¬¢è¿è§‚çœ‹éª°å­ğŸ²é˜Ÿçš„ä½œå“~~~~")
    st.caption("ğŸš€ A Streamlit Chatbot Simulation")
    st.balloons()
    st.write()


def save_chat_history():
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"chat_history_{timestamp}.txt"
    with open(filename, "w", encoding="utf-8") as f:
        for message in st.session_state.messages:
            f.write(f"{message['role']}: {message['content']}\n\n")
    st.success(f"èŠå¤©è®°å½•å·²ä¿å­˜åˆ° {filename}")


def showLLMChatbot():
    st.title("(à¹‘Ìâ€¢âˆ€â€¢à¹‘Ì€)à¸…  LLMèŠå¤©æœºå™¨äºº")

    st.markdown("""
    æ¬¢è¿ä½¿ç”¨æˆ‘ä»¬çš„LLMèŠå¤©æœºå™¨äººï¼taæ”¯æŒå¤šè½®å¯¹è¯å“¦~~~
    """)

    # åˆå§‹åŒ–èŠå¤©å†å²
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # æ·»åŠ æ¸…é™¤èŠå¤©è®°å½•çš„æŒ‰é’®
    col1, col2 = st.columns(2)
    with col1:
        if st.button("æ–°å»ºå¯¹è¯"):
            st.session_state.messages = []
            st.experimental_rerun()

    # æ·»åŠ ä¿å­˜èŠå¤©è®°å½•çš„æŒ‰é’®
    with col2:
        if st.button("ä¿å­˜å¯¹è¯"):
            save_chat_history()

    # æ˜¾ç¤ºèŠå¤©å†å²
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # è·å–ç”¨æˆ·è¾“å…¥
    if prompt := st.chat_input("åœ¨è¿™é‡Œè¾“å…¥ä½ çš„é—®é¢˜..."):
        # å°†ç”¨æˆ·è¾“å…¥æ·»åŠ åˆ°èŠå¤©å†å²
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # æ˜¾ç¤º"æ­£åœ¨æ€è€ƒ"çš„æ¶ˆæ¯
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            message_placeholder.markdown("æ­£åœ¨æ€è€ƒ...")

            # å‡†å¤‡å®Œæ•´çš„å¯¹è¯å†å²
            full_conversation = "\n".join([f"{m['role']}: {m['content']}" for m in st.session_state.messages])

            try:
                # è°ƒç”¨é€šä¹‰åƒé—® API
                gen = Generation()
                response = gen.call(
                    model='qwen-turbo',
                    prompt=full_conversation,
                    api_key=API_KEY
                )

                # å°† AI å›å¤æ·»åŠ åˆ°èŠå¤©å†å²
                assistant_response = response["output"]["text"]
                st.session_state.messages.append({"role": "assistant", "content": assistant_response})
                message_placeholder.markdown(assistant_response)

            except Exception as e:
                error_message = f"æŠ±æ­‰ï¼Œå‘ç”Ÿäº†ä¸€ä¸ªé”™è¯¯ï¼š{str(e)}"
                st.session_state.messages.append({"role": "assistant", "content": error_message})
                message_placeholder.markdown(error_message)


def showmenu2():
    st.title("ğŸ¤–ç¿»è¯‘æœºå™¨äºº")
    st.markdown("""
        ( ã¤â€¢Ì€Ï‰â€¢Ì)ã¤  å¸®åŠ©ä½ å®ç°ä¸åŒè¯­è¨€ä¹‹é—´çš„ç¿»è¯‘
    """)
    # åˆ›å»ºè¯­è¨€é€‰æ‹©å™¨
    input_language = st.selectbox("é€‰æ‹©è¾“å…¥è¯­è¨€:", ["è‹±è¯­", "æ±‰è¯­", "æ³•è¯­", "å¾·è¯­", "è¥¿ç­ç‰™è¯­"])
    output_language = st.selectbox("é€‰æ‹©è¾“å‡ºè¯­è¨€:", ["æ±‰è¯­", "è‹±è¯­", "æ³•è¯­", "å¾·è¯­", "è¥¿ç­ç‰™è¯­"])
    chat_input = st.chat_input("è¯·è¾“å…¥è¦ç¿»è¯‘çš„æ–‡æœ¬ï¼š")
    if chat_input:
        st.chat_message("user").write(chat_input)

        # åˆ›å»ºpromptæ¨¡æ¿
        prompt_template = ChatPromptTemplate.from_messages(
            [
                ("system",
                 f"ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ç¿»è¯‘ï¼Œèƒ½å¤Ÿå°†{{input_language}}ç¿»è¯‘æˆ{{output_language}}ï¼Œå¹¶ä¸”è¾“å‡ºæ–‡æœ¬ä¼šæ ¹æ®ç”¨æˆ·è¦æ±‚çš„ä»»ä½•è¯­è¨€é£æ ¼è¿›è¡Œè°ƒæ•´ã€‚è¯·åªè¾“å‡ºç¿»è¯‘åçš„æ–‡æœ¬ï¼Œä¸è¦æœ‰ä»»ä½•å…¶å®ƒå†…å®¹ã€‚"),
                ("human", f"æ–‡æœ¬ï¼š{chat_input}")
            ]
        )

        # å‡†å¤‡promptå€¼
        prompt_value = prompt_template.format_messages(input_language=input_language,
                                                       output_language=output_language,
                                                       text=chat_input)

        # è®¾ç½®ç¿»è¯‘æ¨¡å‹å¹¶ç”Ÿæˆç¿»è¯‘ç»“æœ
        model = Tongyi(model_name='qwen-max')
        result = model.invoke(prompt_value)

        st.chat_message("assistant").write(f"{result}")


if __name__ == "__main__":
    main()
